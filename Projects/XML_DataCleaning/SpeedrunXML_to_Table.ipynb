{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "When gamers, youtubers, or streamers conduct speedrunning sessions, their =data is saved into a .lss file; a file that stores pre-determined marker time splits in an XML format.\n",
        "\n",
        "Pre-determined markers may be \"Start,\" \"Level 3,\" \"Defeat Final Boss,\" etc. depending on the type of speedrun.\n",
        "\n",
        "The file can prove difficult to work with without prior cleaning, as the data is noded by the pre-determined marker, not by the attempt.\n",
        "\n",
        "That is to say, \"Attempts\" (a.k.a \"runs\") are one node, holding an id, a start time, and a stop time. The \"Segment\" nodes hold the time for all attempts.\n",
        "\n",
        "For example: 100 speedrunning attempts of Super Mario Bros. would have the \"AttemptHistory\" node, which holds 100 child nodes with previously mentioned three pieces of information; the \"Segments\" node would hold a series of child nodes that are listed by the chornological marker order; within \"Segments\" are further child nodes titled \"Segment,\" containing the \"SegmentHistory\" which lists each attempt's time data for that segment.\n",
        "\n",
        "For the data contained in these nested trees, analysis is difficult and thus best converted to tabular or dataframe formats.\n",
        "\n",
        "The following notebook will display parsing of an .lss file, parsing via XPath, then converting into a tabular csv file.\n",
        "\n",
        "Thereafter, some explatory data analysis is performed and some visualizations are displayed."
      ],
      "metadata": {
        "id": "gubbFwpBq2th"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "npQjF5zIXWHU",
        "outputId": "497d8c70-f8d8-43d8-bd8f-16063d4a4b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prologue\n",
            "-Start\n",
            "-Crossing\n",
            "{Forsaken City} Chasm\n",
            "-Start\n",
            "-Intervention\n",
            "{Old Site} Awake\n",
            "-Start\n",
            "-Huge Mess\n",
            "-Elevator Shaft\n",
            "{Celestial Resort} Presidential Suite\n",
            "-Start\n",
            "-Shrine\n",
            "-Old Trail\n",
            "{Golden Ridge} Cliff Face\n",
            "-Start\n",
            "-Cassette\n",
            "-5B Start\n",
            "-Central Chamber\n",
            "-Through the Mirror\n",
            "{Mirror Temple} Mix Master\n",
            "-Start\n",
            "-Lake\n",
            "-Cassette\n",
            "-6B Start\n",
            "-Reflection\n",
            "-Rock Bottom\n",
            "{Reflection} Reprieve\n",
            "-0m\n",
            "-500m\n",
            "-1000m\n",
            "-1500m\n",
            "-2000m\n",
            "-2500m\n",
            "{The Summit} 3000m\n"
          ]
        }
      ],
      "source": [
        "#the .lss file was visually inspected to check out the segment names; some of which start with a hyphen.\n",
        "#let's see the full list of segments\n",
        "\n",
        "#import the relevant package/library\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#import the file and initialize\n",
        "tree = ET.parse('/content/any6bCP.lss')\n",
        "root = tree.getroot()\n",
        "\n",
        "#print split names\n",
        "for seg in root.findall('.//Segment'):\n",
        "    print(seg.find('Name').text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyphens seen on Segment names will prove annoying in tabular form, so we'll trim those now."
      ],
      "metadata": {
        "id": "esPABUr9t_ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's run a for loop to remove the hyphen from affected segment names\n",
        "\n",
        "for seg in root.findall('.//Segment'):\n",
        "  title_hyphen = seg.find('Name')\n",
        "  if title_hyphen is not None:\n",
        "    title_text = title_hyphen.text or \"\"\n",
        "    if title_text.startswith(\"-\"):\n",
        "      title_hyphen.text = title_text[1:]\n",
        "\n",
        "#we'll save as a new file for preservation\n",
        "tree.write(\"splits_updated.xml\", encoding=\"utf-8\", xml_declaration=True)"
      ],
      "metadata": {
        "id": "DeItCxXgYoFV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll move onto converting the .lss file into a tabular format (csv).\n",
        "\n",
        "The original stakeholder of this project wanted the data converted into a tabular format with observations as 1 run, and features as segment names (e.g. ascending markers from start to finish of the speedrun). The cell value is time in duration of the run in milliseconds (ex. 5:24:00 of game time has passed, displayed as 324000 milliseconds).  \n",
        "\n",
        "Thus, we'll extract the segment names to be column names, and we'll order them appropriately.\n",
        "The time splits in the .lss file are formatted as hh:mm:ss.fraction, so we'll parse them to convert them to milliseconds.\n",
        "Then we'll add up the time across the attempt.\n",
        "\n",
        "To finish, we'll write the data to a csv."
      ],
      "metadata": {
        "id": "eDbrtcTluZeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import relevant packages / libraries\n",
        "\n",
        "import re\n",
        "from collections import defaultdict, OrderedDict\n",
        "import csv\n",
        "\n",
        "#use the new file with updated segment names and prep the output file\n",
        "\n",
        "LSS_PATH = \"/content/splits_updated.xml\"\n",
        "CSV_OUT = \"attempts_cumulative_ms.csv\"\n",
        "\n",
        "#helper functions\n",
        "\n",
        "#some xml files will have namespaces that, parsed into python, don't translate well.\n",
        "#this function splits on the final } of the namespace and extracts just the localname\n",
        "\n",
        "def local(tag):\n",
        "    \"\"\"Return tag localname without namespace.\"\"\"\n",
        "    return tag.rsplit('}', 1)[-1]\n",
        "\n",
        "#in order to parse the formatting of the time splits in the .lss, we'll use regular expression\n",
        "#to convert from strings to numerical values\n",
        "\n",
        "_time_re = re.compile(\n",
        "    r'^(?:(?P<days>\\d+)\\.)?(?P<h>\\d{1,2}):(?P<m>\\d{2}):(?P<s>\\d{2})(?:\\.(?P<f>\\d{1,9}))?$'\n",
        ")\n",
        "\n",
        "#we'll convert the time values to a millisecond integer\n",
        "\n",
        "def parse_time_to_ms(text):\n",
        "    \"\"\"\n",
        "    Parse LiveSplit time like:\n",
        "    '00:12:34.567', '1.02:03:04.789' (d.hh:mm:ss.fff) â†’ milliseconds (int).\n",
        "    Returns None if text is empty/invalid.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    m = _time_re.match(text.strip())\n",
        "    if not m:\n",
        "        return None\n",
        "    days = int(m.group(\"days\") or 0)\n",
        "    h = int(m.group(\"h\"))\n",
        "    mi = int(m.group(\"m\"))\n",
        "    s = int(m.group(\"s\"))\n",
        "    frac = m.group(\"f\") or \"0\"\n",
        "    #any fractional seconds will be normalized (truncate/exact if 3+ digits)\n",
        "    #e.g., \"5\"->500ms, \"56\"->560ms\n",
        "    if len(frac) < 3:\n",
        "        ms = int(frac.ljust(3, \"0\"))\n",
        "    else:\n",
        "        ms = int(frac[:3])\n",
        "    total_ms = (((days*24 + h)*60 + mi)*60 + s)*1000 + ms\n",
        "    return total_ms\n",
        "\n",
        "#load the XML and initiatize the root\n",
        "\n",
        "tree = ET.parse(LSS_PATH)\n",
        "root = tree.getroot()\n",
        "\n",
        "#get ordered segment names\n",
        "\n",
        "segments_node = next((n for n in root.iter() if local(n.tag) == \"Segments\"), None)\n",
        "if segments_node is None:\n",
        "    raise RuntimeError(\"No <Segments> found in LSS.\")\n",
        "\n",
        "segment_names = []\n",
        "for seg in segments_node:\n",
        "    if local(seg.tag) == \"Segment\":\n",
        "        name_node = next((c for c in seg if local(c.tag) == \"Name\"), None)\n",
        "        segment_names.append(name_node.text if name_node is not None else \"\")\n",
        "\n",
        "#collect per-attempt per-segment, segment times\n",
        "#SegmentHistory/Time @id + GameTime text\n",
        "attempt_segment_ms = defaultdict(dict)  # {attempt_id: {segment_name: ms}}\n",
        "\n",
        "for seg in segments_node:\n",
        "    if local(seg.tag) != \"Segment\":\n",
        "        continue\n",
        "    seg_name = next((c.text for c in seg if local(c.tag) == \"Name\"), \"\")\n",
        "    hist = next((c for c in seg if local(c.tag) == \"SegmentHistory\"), None)\n",
        "    if hist is None:\n",
        "        continue\n",
        "    for t in hist:\n",
        "        if local(t.tag) != \"Time\":\n",
        "            continue\n",
        "        att_id = t.get(\"id\")\n",
        "        rt = next((c.text for c in t if local(c.tag) == \"GameTime\"), None)\n",
        "        ms = parse_time_to_ms(rt)\n",
        "        if att_id and ms is not None:\n",
        "            attempt_segment_ms[att_id][seg_name] = ms\n",
        "\n",
        "#compute cumulative per attempt in segment order\n",
        "attempt_ids_sorted = sorted(attempt_segment_ms.keys(), key=lambda x: int(x))\n",
        "rows = OrderedDict()  # {attempt_id: [cumulative_ms per segment order]}\n",
        "for att_id in attempt_ids_sorted:\n",
        "    cum = 0\n",
        "    row = []\n",
        "    for seg_name in segment_names:\n",
        "        seg_ms = attempt_segment_ms[att_id].get(seg_name)\n",
        "        if seg_ms is None:\n",
        "            row.append(None)  # missing\n",
        "        else:\n",
        "            cum += seg_ms\n",
        "            row.append(cum)\n",
        "    rows[att_id] = row\n",
        "\n",
        "#write output csv\n",
        "with open(CSV_OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"attempt_id\"] + segment_names)\n",
        "    for att_id, vals in rows.items():\n",
        "        w.writerow([att_id] + vals)\n",
        "\n",
        "print(f\"Wrote {CSV_OUT} with {len(rows)} attempts and {len(segment_names)} segments.\")\n",
        "\n",
        "#print a row sample to verify\n",
        "for att_id, vals in rows.items():\n",
        "    numbers = \", \".join(\"\" if v is None else str(v) for v in vals)\n",
        "    print(f\"{att_id}: {numbers}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UruTqC-YXNj",
        "outputId": "5b95863b-22d1-49fa-8560-dc538fadde37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote attempts_cumulative_ms.csv with 3796 attempts and 35 segments.\n",
            "-1: , 21930, 46138, , 68068, , , 89998, , , , 111928, 128639, , 171427, 193357, , , , 235975, , 257905, , , , , 312644, 384741, , , , , , , \n"
          ]
        }
      ]
    }
  ]
}
